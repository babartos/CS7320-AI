{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Search: Playing Connect 4\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undegraduates 10, graduate students 11\n",
    "\n",
    "Complete this notebook and submit it. The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You will implement different versions of agents that play Connect 4:\n",
    "\n",
    "> \"Connect 4 is a two-player connection board game, in which the players choose a color and then take turns dropping colored discs into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\" (see [Connect Four on Wikipedia](https://en.wikipedia.org/wiki/Connect_Four))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Defining the Search Problem [1 point]\n",
    "\n",
    "Define the components of the search problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Initial state</b>: The initial state is the state the agent starts in (Russel 64). In the context of a game, a state, <i>s</i>, can be defined as a function of (position, board). For Connect Four, the initial state will be written as <i>s0</i>, where s0 = empty board. Using the earlier described function notation, the game can also be written as (position=none, board=empty).\n",
    "\n",
    "<b>Actions</b>: Given a state <i>s</i>, actions(s) are the finite set of options that can be executed in this state <i>s</i> (Russel 64). In terms of Connect Four, the actions are playing in one of the empty spaces (actions are everywhere the player can move). In Connect Four, there is one valid action/move/play per column.\n",
    "\n",
    "<b>Transition model</b>: \n",
    "A transition model is a way to describe what each action does. In other words, transition models maps a state and action to a resulting state. The environment of ConnectFour is non-deterministic, and a transition model is needed to describe the uncertainty and results of each action. The transition model will return the resulting state from the action <i>a</i>. The transition model will return the board after the piece has been played at the location designated by the agent. The transition model returns the result of an agent's action from their turn. \n",
    "\n",
    "<b>Goal State</b>:  The goal state is the desired state of the agent. For Connect Four, the goal state is victory achieved by having 4 pieces in a row either horizontally, vertically, or diagonally. Both a loss or a draw are considered a failure and not apart of the goal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the search space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the exact state space/search space of Conncet Four is unknown, we can do a quick estimation to get an idea of the state space. \n",
    "\n",
    "We will model the enviornment as a permutation (Similar to the class handout: \"how many ways can we order/arrange the objects in the environment\"). Each space has three possible options:  \"x piece\", \"y piece\", and empty (n == 3). The total number of slots is 42 because the board size is 6x7 (r == 42). However we know that not all these states might be reachable because the path leading to them would result in a terminal state/ end the game. Therefore the real state space will be smaller than the theoritical calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109418989131512359209"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#permutation state space calcualtion as explained above: 3^42 (3 = r, n = 32)\n",
    "3**42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefor, the actual state space is  \"< 109418989131512359209\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ The search space for a $6 \\times 7$ board is large. You can experiment with smaller boards (the smallest is $4 times \\4$) and/or changing the winning rule to connect 3 instead of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Game Environment and Random Agent [2 point]\n",
    "\n",
    "Use a numpy character array as the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of colors for the players use 'x' and 'o' to represent the players. Make sure that your agent functions all have the from: `agent_type(board, player = 'x')`, where board is the current board position and player is the player whose next move it is and who the agent should play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the board and helper functions for:\n",
    "\n",
    "* The transition model (result).\n",
    "* The utility function.\n",
    "* Check for terminal states.\n",
    "* A check for available actions.\n",
    "* A function to visualize the board.\n",
    "\n",
    "Make sure that all these functions work with boards of different sizes.\n",
    "\n",
    "Implement an agent that plays randomly and let two random agents play against each other 1000 times. How often does each player win? Is the result expected? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Helper Functions:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements:\n",
    "import numpy as np\n",
    "import random\n",
    "#global declarations:\n",
    "shape=(6, 7)\n",
    "winCondition = 4 #how many pieces connected in a row to win\n",
    "player1Score = 0\n",
    "player1Wins = 0 #total number of wins\n",
    "player2Score = 0\n",
    "player2Wins = 0\n",
    "draws = 0\n",
    "\n",
    "#Use a numpy character array as the board.\n",
    "def empty_board(): \n",
    "    global shape\n",
    "    return np.full(shape=shape, fill_value=' ')\n",
    "\n",
    "#helper function determining if a space is valid or not\n",
    "def isValid(x,y):\n",
    "    global shape\n",
    "    if(x >= 0 and y >=0):\n",
    "        if (x <= shape[0] - 1 and y <= shape[1] - 1):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#TERMINAL STATES: test for terminal states, or win conditions, that would end the game:\n",
    "def terminalStates(board):\n",
    "    arr = [\"right\", \"up\", \"down\", \"left\", \"diagnoalUpRight\", \"diagnoalUpLeft\", \"diagnoalLowLeft\", \"diagnoalLowRight\"]\n",
    "    global winCondition\n",
    "    for i in range(0, len(board)):\n",
    "        row = board[i]\n",
    "        for j in range(0, len(row)):  # for each space\n",
    "            currSpace = board[i][j]\n",
    "            if (currSpace != 'x' and currSpace != 'o'):\n",
    "                continue\n",
    "            #each direction from current valid space\n",
    "            for direction in arr:\n",
    "                # check to the right:\n",
    "                counter = 1\n",
    "                colPos = j\n",
    "                rowPos = i\n",
    "                while (counter <= winCondition):\n",
    "                    #----- determine what direction -----\n",
    "                    if(direction == \"right\"):\n",
    "                        colPos = colPos + 1\n",
    "                    if(direction == \"left\"):\n",
    "                        colPos = colPos - 1\n",
    "                    if(direction == \"up\"):\n",
    "                        rowPos = rowPos + 1\n",
    "                    if(direction == \"down\"):\n",
    "                        rowPos = rowPos - 1\n",
    "                    # move in a diagnol direction\n",
    "                    if(direction == \"diagnoalUpRight\"):\n",
    "                        colPos = colPos + 1\n",
    "                        rowPos = rowPos + 1\n",
    "                    if(direction == \"diagnoalUpLeft\"):\n",
    "                        colPos = colPos + 1\n",
    "                        rowPos = rowPos - 1\n",
    "                    if(direction == \"diagnoalLowRight\"):\n",
    "                        rowPos = rowPos - 1\n",
    "                        colPos = colPos + 1\n",
    "                    if(direction == \"diagnoalLowLeft\"):\n",
    "                        rowPos = rowPos - 1\n",
    "                        colPos = colPos - 1\n",
    "                    #----- end direction ---------\n",
    "                    if (isValid(rowPos, colPos)):  # if space exists\n",
    "                        if (board[rowPos][colPos] == currSpace):  # continuing with valid win condition\n",
    "                            counter = counter + 1\n",
    "                            if (counter == winCondition): #win condition reached\n",
    "                                return currSpace\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "    #---- end for loop ----\n",
    "    return 'none'\n",
    "\n",
    "\n",
    "#return a List of all the available moves for a player in the environment. \n",
    "def availableActions(board):\n",
    "    validSpaces = []\n",
    "    for i in range(0, shape[1]): #iterate through columns -> 1 valid move per column (typically)\n",
    "        for j in range(shape[0] - 1, -1, -1):\n",
    "            currSpace = board[j][i]\n",
    "            if (currSpace != 'x' and currSpace != 'o'):\n",
    "                location = (j, i)\n",
    "                validSpaces.append(location)\n",
    "                break\n",
    "    return validSpaces\n",
    "\n",
    "\n",
    "#print the board to the screen# transition model models the results of an action: If I place token, how does the board look now\n",
    "def visualizeBoard(board):\n",
    "    print(board)\n",
    "    \n",
    "#assign the winner. +1 for a win, 0 for a draw, -1 for a loss\n",
    "def utilityFunction(player):\n",
    "    # keep track of total wins\n",
    "    global player1Wins\n",
    "    global player2Wins\n",
    "    #\n",
    "    global player2Score\n",
    "    global player1Score\n",
    "    global draws\n",
    "    if('x' == player):\n",
    "        player1Score = player1Score + 1\n",
    "        player2Score -= 1\n",
    "        player1Wins += 1\n",
    "    elif('o' == player):\n",
    "        player1Score -= 1\n",
    "        player2Score += 1\n",
    "        player2Wins += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "    #----- end function ----\n",
    "\n",
    "#switch player\n",
    "def switch_player(player):\n",
    "    if player == 'x':\n",
    "        return 'o'\n",
    "    else:\n",
    "        return 'x'\n",
    "\n",
    "#clear scores from the utility function:\n",
    "def clearScores():\n",
    "    global player1Score\n",
    "    global player2Score\n",
    "    global player1Wins\n",
    "    global player2Wins\n",
    "    global draws\n",
    "    player1Score = player2Score = 0\n",
    "    player1Wins = player2Wins = 0\n",
    "    draws = 0\n",
    "    \n",
    "    \n",
    "# transition model models the results of an action: If I place token, how does the board look now \n",
    "def transitionModel(board, location, player):\n",
    "    try:\n",
    "        board[location[0]][location[1]] = player\n",
    "    except:\n",
    "        print(\"error, check indexing\")\n",
    "        \n",
    "\n",
    "#statically evaluate a space\n",
    "# statically evaluate a space\n",
    "def evalSpace(board, space, currPlayer):\n",
    "    arr = [\"right\", \"up\", \"down\", \"left\", \"diagnoalUpRight\", \"diagnoalUpLeft\", \"diagnoalLowLeft\", \"diagnoalLowRight\"]\n",
    "    maxCount = 0\n",
    "    for direction in arr:\n",
    "        rowPos = space[0]\n",
    "        colPos = space[1]\n",
    "        count = 0\n",
    "        for x in range(0, winCondition):\n",
    "            # ----- determine what direction -----\n",
    "            if (direction == \"right\"):\n",
    "                colPos = colPos + 1\n",
    "            if (direction == \"left\"):\n",
    "                colPos = colPos - 1\n",
    "            if (direction == \"down\"):\n",
    "                rowPos = rowPos + 1\n",
    "            if (direction == \"up\"):\n",
    "                rowPos = rowPos - 1\n",
    "            # move in a diagnol direction\n",
    "            if (direction == \"diagnoalUpRight\"):\n",
    "                colPos = colPos + 1\n",
    "                rowPos = rowPos + 1\n",
    "            if (direction == \"diagnoalUpLeft\"):\n",
    "                colPos = colPos + 1\n",
    "                rowPos = rowPos - 1\n",
    "            if (direction == \"diagnoalLowRight\"):\n",
    "                rowPos = rowPos - 1\n",
    "                colPos = colPos + 1\n",
    "            if (direction == \"diagnoalLowLeft\"):\n",
    "                rowPos = rowPos - 1\n",
    "                colPos = colPos - 1\n",
    "            # ----- end direction ---------\n",
    "            count = count + 1\n",
    "            #if space is invalid breakout of loop\n",
    "            if (not isValid(rowPos, colPos)):\n",
    "                if (count > maxCount):\n",
    "                    maxCount = count\n",
    "                break\n",
    "            currSpace = board[rowPos][colPos]\n",
    "            # if not the current player, breakout of the loop\n",
    "            if (currSpace != currPlayer):\n",
    "                if (count > maxCount):\n",
    "                    maxCount = count\n",
    "                break\n",
    "    return maxCount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random Agent Implementation:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement an agent that plays randomly and let two random agents play against each other 1000 times. \n",
    "#How often does each player win? Is the result expected? \n",
    "def agent_random(board, player):\n",
    "    spaces = availableActions(board)\n",
    "    if(len(spaces) == 0):\n",
    "        return spaces\n",
    "    return random.choice(spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_random(n = 1000):\n",
    "    player1 = 'x'\n",
    "    player2 = 'o'\n",
    "    for i in range(0,n):\n",
    "        currPlayer = player1\n",
    "        board = empty_board()\n",
    "        result = \"none\"\n",
    "        while(result == \"none\"):\n",
    "            myChoice = agent_random(board, currPlayer)\n",
    "            if(len(myChoice) < 2):\n",
    "                break\n",
    "            transitionModel(board, myChoice, currPlayer)\n",
    "            #switch players:\n",
    "            if currPlayer == player1:\n",
    "                currPlayer = player2\n",
    "            else:\n",
    "                currPlayer = player1\n",
    "            result = terminalStates(board)\n",
    "        utilityFunction(result)\n",
    "        #visualizeBoard(board)\n",
    "    #output results to the console\n",
    "    print('player 1 wins: ', player1Wins)\n",
    "    print('player 2 wins: ', player2Wins)\n",
    "    print('total draws:', draws)\n",
    "    clearScores()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 wins:  562\n",
      "player 2 wins:  435\n",
      "total draws: 3\n"
     ]
    }
   ],
   "source": [
    "play_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of 1,000 trials of two randow agents are as follows: player 1 winning 540 times and player 2 winning 457 times out of a thousand. These results are as expected because it is close to 50/50 or random chance. Player 1 has a slight advantage to player2. This discrepency may be attributed to player 1 always moving first. Always moving first means player 1 has a slighty higher chance of winning all its games when compared with player 2 always moving second. Overall, the results make logical sesne and are understandable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Minimax Search with Alpha-Beta Pruning [4 points]\n",
    "\n",
    "### Implement the search starting from a given board and specifying the player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimax search:\n",
    "#PARAMATERS:\n",
    "# alpha = lower bound, beta = upperbound\n",
    "# board, space = state information. space is a pair\n",
    "# currPlayer = 'x' , 'o'\n",
    "# maximizingPlayer = True, False (maxmize if True, minimize if false)\n",
    "\n",
    "def minimax(board, currPlayer, depth):\n",
    "    #alpha = +infinity. beta = -infinity\n",
    "    space = (0,0) #space = initial starting space\n",
    "    myEval = maxValue(board, space, currPlayer, depth, 1000, -1000)\n",
    "    #modify 3 to MAX value for non-depth resitrected (board too large to do this right nwo)\n",
    "    return myEval[1]\n",
    "\n",
    "#FUNCTION RETURNING MAXIMUM VALUE\n",
    "def maxValue(board, space, currPlayer, depth, alpha, beta):\n",
    "    #if terminal states OR depth is reached --> terminate\n",
    "    result = terminalStates(board)\n",
    "    if(depth == 0 or result != \"none\"): #if terminal states:\n",
    "        evalResult = (evalSpace(board, space, currPlayer), space)\n",
    "        return evalResult\n",
    "\n",
    "    # actual maximum evaluation:\n",
    "    maxEval = -1  #intilize to negative infinity\n",
    "    maxMove = (0,0)\n",
    "    children = availableActions(board)\n",
    "    for child in children:\n",
    "        evaluation = minValue(board, child, switch_player(currPlayer), (depth - 1), alpha, beta )\n",
    "        #evaluation[0] = stativEvaluation of how good a move is\n",
    "        #evaluation[1] = actual move / space coordiantes\n",
    "        if(maxEval < evaluation[0]): #if new value is greater than maxVolume\n",
    "            maxEval = evaluation[0]\n",
    "            maxMove = evaluation[1]\n",
    "            alpha = max(alpha, maxEval)\n",
    "        if(maxEval <= beta):\n",
    "            x = 1\n",
    "            break\n",
    "    return (maxEval, maxMove)\n",
    "\n",
    "#FUNCTION RETURNING MINMUM VALUE\n",
    "def minValue(board, space, currPlayer, depth, alpha, beta):\n",
    "    # --- if terminal states OR depth is reached ---\n",
    "    result = terminalStates(board)\n",
    "    if(depth == 0 or result != \"none\"): #if terminal states:\n",
    "        evalResult = (evalSpace(board, space, switch_player(currPlayer) ), space)\n",
    "        return evalResult\n",
    "\n",
    "    # actual min evaluation:\n",
    "    minEval = 1000\n",
    "    minMove = (0,0)\n",
    "    children = availableActions(board)\n",
    "    for child in children:\n",
    "        evaluation = maxValue(board, child, switch_player(currPlayer), (depth - 1), alpha, beta )\n",
    "        #evaluation[0] = stativEvaluation of how good a move is\n",
    "        #evaluation[1] = space coordiantes\n",
    "        if(minEval > evaluation[0]): #if new value is less than maximum value\n",
    "            minEval = evaluation[0]\n",
    "            minMove = evaluation[1]\n",
    "            beta = min(beta, minEval)\n",
    "        if(minEval >= alpha):\n",
    "            x = 1\n",
    "            break\n",
    "    return (minEval, minMove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with some manually created boards (at least 5) to check if the agent spots wining opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Board 1: Looking right/left to find solution. Agent finds winning move (5,3) to get 4 x's in a row. Causes terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " ['x' 'x' 'x' ' ' ' ' ' ' ' ']]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "#do minimax:\n",
    "board = empty_board()\n",
    "board[5][0] = board[5][1] = board[5][2] = 'x'\n",
    "move = minimax(board, 'x', 5)\n",
    "#output results:\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Board# 2a: Looking up/down to find solution. Agent finds winning move (2,0) to get 4 x's in a vertical column. Causes terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " ['x' ' ' ' ' 'o' ' ' ' ' ' ']\n",
      " ['x' ' ' ' ' 'o' ' ' ' ' ' ']\n",
      " ['x' ' ' ' ' 'o' ' ' ' ' ' ']]\n",
      "(2, 0)\n"
     ]
    }
   ],
   "source": [
    "board = empty_board()\n",
    "board[5][0] = board[4][0] = board[3][0] = 'x'\n",
    "board[5][3] = board[4][3] = board[3][3] = 'o' \n",
    "move = minimax(board, 'x', 5)\n",
    "#output results:\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Board 2b: Looking up/down to find solution for 'o'. Agent finds winning move (2,3) to get 4 o's in a vertical column. Causes terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " ['x' ' ' ' ' 'o' ' ' ' ' ' ']\n",
      " ['x' ' ' ' ' 'o' ' ' ' ' ' ']\n",
      " ['x' ' ' ' ' 'o' ' ' ' ' ' ']]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "board = empty_board()\n",
    "board[5][0] = board[4][0] = board[3][0] = 'x'\n",
    "board[5][3] = board[4][3] = board[3][3] = 'o' \n",
    "move = minimax(board, 'o', 5)\n",
    "#output results:\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Board 3: Looking diaganol to find solution for 'x'. Agent finds winning move (5,3) to get 4 x's in a diagnol line. Causes terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " ['x' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " ['o' 'x' ' ' ' ' ' ' ' ' ' ']\n",
      " ['o' 'o' 'x' ' ' ' ' ' ' ' ']\n",
      " ['o' 'o' 'o' ' ' ' ' ' ' ' ']]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "#intilize board:\n",
    "board = empty_board()\n",
    "board[2][0] = board[3][1] = board[4][2] = 'x'\n",
    "board[5][0] = board[4][0] = board[3][0] = 'o' \n",
    "board[5][1] = board[4][1] = board[5][2] = 'o' \n",
    "#output results:\n",
    "move = minimax(board, 'x', 5)\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Board 4: Looking right/left to find solution for 'o'. Agent finds winning move (5,3) to get 4 x's in a line. Causes terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " ['x' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " ['o' 'x' ' ' ' ' ' ' ' ' ' ']\n",
      " ['o' 'o' 'x' ' ' ' ' ' ' ' ']\n",
      " ['o' 'o' 'o' ' ' ' ' ' ' ' ']]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "#intilize board:\n",
    "board = empty_board()\n",
    "board[2][0] = board[3][1] = board[4][2] = 'x'\n",
    "board[5][0] = board[4][0] = board[3][0] = 'o' \n",
    "board[5][1] = board[4][1] = board[5][2] = 'o' \n",
    "#output results:\n",
    "move = minimax(board, 'o', 5)\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Board 5: Agent finds winning move (2,6) to get 4 o's in a vertical column. Causes terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' 'x' 'o']\n",
      " [' ' ' ' ' ' ' ' 'x' ' ' 'o']\n",
      " [' ' ' ' ' ' 'x' ' ' ' ' 'o']]\n",
      "(2, 6)\n"
     ]
    }
   ],
   "source": [
    "#intilize board:\n",
    "board = empty_board()\n",
    "board[5][3] = board[4][4] = board[3][5] ='x'\n",
    "board[5][6] = board[4][6] = board [3][6] = 'o'\n",
    "\n",
    "#output results:\n",
    "move = minimax(board, 'o', 5)\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n",
      "(3, 0)\n",
      "Wall time: 30.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Your code/ answer goes here.\n",
    "shape=(4, 4)\n",
    "winCondition = 3 #how many pieces connected in a row to win\n",
    "\n",
    "\n",
    "board = empty_board()\n",
    "move = minimax(board, 'x', 10)\n",
    "#output results:\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n",
      "(4, 0)\n",
      "Wall time: 5min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Your code/ answer goes here.\n",
    "shape=(5, 5)\n",
    "winCondition = 3 #how many pieces connected in a row to win\n",
    "\n",
    "\n",
    "board = empty_board()\n",
    "move = minimax(board, 'x', 10)\n",
    "#output results:\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mminimax\u001b[1;34m(board, currPlayer, depth)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#alpha = +infinity. beta = -infinity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#space = initial starting space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmyEval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrPlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#modify 3 to MAX value for non-depth resitrected (board too large to do this right nwo)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmyEval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mmaxValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswitch_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m#evaluation[0] = stativEvaluation of how good a move is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#evaluation[1] = actual move / space coordiantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mminValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswitch_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;31m#evaluation[0] = stativEvaluation of how good a move is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#evaluation[1] = space coordiantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mmaxValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswitch_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m#evaluation[0] = stativEvaluation of how good a move is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#evaluation[1] = actual move / space coordiantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mminValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswitch_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;31m#evaluation[0] = stativEvaluation of how good a move is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#evaluation[1] = space coordiantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mmaxValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswitch_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m#evaluation[0] = stativEvaluation of how good a move is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#evaluation[1] = actual move / space coordiantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mminValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswitch_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;31m#evaluation[0] = stativEvaluation of how good a move is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#evaluation[1] = space coordiantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mmaxValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswitch_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m#evaluation[0] = stativEvaluation of how good a move is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#evaluation[1] = actual move / space coordiantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mminValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailableActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswitch_player\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;31m#evaluation[0] = stativEvaluation of how good a move is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m#evaluation[1] = space coordiantes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b5fb16bf22d>\u001b[0m in \u001b[0;36mmaxValue\u001b[1;34m(board, space, currPlayer, depth, alpha, beta)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterminalStates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"none\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#if terminal states:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mevalResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mevalSpace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrPlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mevalResult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-0cb12b3efd9c>\u001b[0m in \u001b[0;36mevalSpace\u001b[1;34m(board, space, currPlayer)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mcolPos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwinCondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[1;31m# ----- determine what direction -----\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"right\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Your code/ answer goes here.\n",
    "shape=(6, 7)\n",
    "winCondition = 3 #how many pieces connected in a row to win\n",
    "\n",
    "\n",
    "board = empty_board()\n",
    "move = minimax(board, 'x', 8)\n",
    "#output results:\n",
    "visualizeBoard(board)\n",
    "print(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show that as the size of the board increases (or size of search space increases) the execution time of minimax search increases exponentially. As the size of the board increases the number of children of each space/node increases by a large amount. Meaning, for each extra child alot more function calls/recursion will be needed. The searches are also somewhat depth limited. It would take along time to find a terminal state/loop through all terminal states for a larger board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move ordering\n",
    "\n",
    "Describe and implement a simple move ordering strategy. How does this strategy influence the time it takes to \n",
    "make a move?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let the Minimax Search agent play a random agent on a small board. Analyze wins, losses and draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Heuristic Alpha-Beta Tree Search [3 points] \n",
    "\n",
    "### Heuristic evaluation function\n",
    "\n",
    "Define and implement a heuristic evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting off search \n",
    "\n",
    "Modify your Minimax Search with Alpha-Beta Pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the same manually created boards as above to check if the agent spots wining opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pruning\n",
    "\n",
    "Add forward pruning to the cutoff search where you do not consider moves that have a low evaluation value after a shallow search \n",
    "(way smaller than the cuttoff value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let two heuristic search agents (different cutoff depth, different heuristic evaluation function or different forward pruning) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge task [+ 1 bonus point]\n",
    "\n",
    "Find another student and let your best agent play against the other student's best player. We will set up a class tournament on Canvas. This tournament will continue after the submission deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduate student advanced task: Pure Monte Carlo Search and Best First Move [1 point]\n",
    "\n",
    "__Undergraduate students:__ This is a bonus task you can attempt if you like [+1 Bonus point].\n",
    "\n",
    "### Pure Monte Carlos Search\n",
    "\n",
    "Implement Pure Monte Carlo Search and investigate how this search performs on the test boards that you have used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best First Move\n",
    "\n",
    "How would you determine what the best first move is? You can use Pure Monte Carlo Search or any algorithms \n",
    "that you have implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
